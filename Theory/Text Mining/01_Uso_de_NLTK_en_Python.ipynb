{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Uso de NLTK"
      ],
      "metadata": {
        "id": "fXjOv0sB_N8b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SACapOdX9sQ8",
        "outputId": "e895f38a-5a55-4ae0-b1ca-2a8aa64dd3d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cess_esp.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import nltk\n",
        "#descargar el corpus en español\n",
        "nltk.download(\"cess_esp\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Expresiones regulares"
      ],
      "metadata": {
        "id": "xTIBg5zG_UeZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "corpus = nltk.corpus.cess_esp.sents()\n",
        "\n",
        "print(corpus)\n",
        "print(len(corpus))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85rav7DS9-z9",
        "outputId": "6cfd2846-2e8e-42e6-8bc8-12a0f0fc924c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana', 'Electricidad_Águila_de_Altamira', '-Fpa-', 'EAA', '-Fpt-', ',', 'creada', 'por', 'el', 'japonés', 'Mitsubishi_Corporation', 'para', 'poner_en_marcha', 'una', 'central', 'de', 'gas', 'de', '495', 'megavatios', '.'], ['Una', 'portavoz', 'de', 'EDF', 'explicó', 'a', 'EFE', 'que', 'el', 'proyecto', 'para', 'la', 'construcción', 'de', 'Altamira_2', ',', 'al', 'norte', 'de', 'Tampico', ',', 'prevé', 'la', 'utilización', 'de', 'gas', 'natural', 'como', 'combustible', 'principal', 'en', 'una', 'central', 'de', 'ciclo', 'combinado', 'que', 'debe', 'empezar', 'a', 'funcionar', 'en', 'mayo_del_2002', '.'], ...]\n",
            "6030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplanar una lista compuesta de sublistas\n",
        "\n",
        "flatten = [w for l in corpus for w in l]\n"
      ],
      "metadata": {
        "id": "O0bP1_OCAs6x"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(flatten)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_vp-Wq7BhVq",
        "outputId": "e2e1f227-4f46-410b-b403-679802d85468"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "192686"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(flatten[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98m5FjA-Bq0d",
        "outputId": "dd5b75bb-8916-4d3c-e1b4-4c305614f79c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['El', 'grupo', 'estatal', 'Electricité_de_France', '-Fpa-', 'EDF', '-Fpt-', 'anunció', 'hoy', ',', 'jueves', ',', 'la', 'compra', 'del', '51_por_ciento', 'de', 'la', 'empresa', 'mexicana']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicacion de expresiones regulares sobre la variable **flatten**:\n",
        "\n",
        "re.search(p,s)\n",
        "\n",
        "p = patron de busqueda\n",
        "s = cadena donde buscar"
      ],
      "metadata": {
        "id": "k2rghBc8CFNK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Meta-caracteres basicos\n",
        "\n",
        "arr = [w for w in flatten if re.search(\"es\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIjtvgOFCMX2",
        "outputId": "ed8f9abc-9686-44b4-b883-b2cc4d1038f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['estatal', 'jueves', 'empresa', 'centrales', 'francesa']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [w for w in flatten if re.search(\"es$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rguHBFA1DcnO",
        "outputId": "ef4251e7-aa7b-4842-f0c0-10d9e6cf361b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['jueves', 'centrales', 'millones', 'millones', 'dólares']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arr = [w for w in flatten if re.search(\"^..j..t..$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf0timi3DqWt",
        "outputId": "357dd587-e314-4990-e056-8b7c279b4081"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tajantes']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aplicacion de rangos\n",
        "# [a-z], [A-Z], [0-9]\n",
        "arr = [w for w in flatten if re.search(\"^[ghi][mno][jlk][def]$\", w)]\n",
        "\n",
        "arr[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEV2X6vLEUxR",
        "outputId": "dea4202b-4f18-4e1e-b02c-97a3326e7ad7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['golf', 'golf']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizacion del Texto"
      ],
      "metadata": {
        "id": "oSAxM7TN88XV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(1) Tokenizacion"
      ],
      "metadata": {
        "id": "oiR8tDZ-92Nf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"\"\"Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
        "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\"\"\"\n",
        "\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v7zDPPIS-VCe",
        "outputId": "886762e5-109c-40b7-c274-7cf6b2412712"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuando sea presidente del mundo (imaginaba en mi cabeza) no tendre que preocuparme por tantas tonteras.\n",
            "Era solo un niño de 10 años, pero pensaba que podria ser cualquier cosa en su imaginacion...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 1\n",
        "print(re.split(r' ', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42pz-deq_RSd",
        "outputId": "b2a24cb0-9280-4c10-98e6-f891c67bc76d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.\\nEra', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 2:\n",
        "print(re.split(r'[ \\t\\n]+', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXCUO-_J_8Fo",
        "outputId": "f4e9cfe9-2a49-4b44-d201-2871862f1f06"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', '(imaginaba', 'en', 'mi', 'cabeza)', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras.', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años,', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Caso 3:\n",
        "# \\W omite todos los caracteres que nos sean letras, digitos o guiones\n",
        "print(re.split(r'[ \\W\\t\\n]+', texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rd1GqRxAaVp",
        "outputId": "a51f8b08-b123-4b23-a3bd-9bced5071f6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Cuando', 'sea', 'presidente', 'del', 'mundo', 'imaginaba', 'en', 'mi', 'cabeza', 'no', 'tendre', 'que', 'preocuparme', 'por', 'tantas', 'tonteras', 'Era', 'solo', 'un', 'niño', 'de', '10', 'años', 'pero', 'pensaba', 'que', 'podria', 'ser', 'cualquier', 'cosa', 'en', 'su', 'imaginacion', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"En los E.U. esa hamburguesa cuesta $15.50\"\n",
        "print(re.split(r'[ \\W\\t\\n]+',texto))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JRxhZQ_9ARl",
        "outputId": "2f07f566-6f35-4428-8bf8-19ab01f5216b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['En', 'los', 'E', 'U', 'esa', 'hamburguesa', 'cuesta', '15', '50']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'''(?x)                 # set flag to allow verbose regexps\n",
        "              (?:[A-Z]\\.)+         # abbreviations, e.g. U.S.A.\n",
        "              | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
        "              | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
        "              | \\.\\.\\.             # ellipsis\n",
        "              | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
        "'''"
      ],
      "metadata": {
        "id": "JMFBeGqhBi5f"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.regexp_tokenize(texto, pattern)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLYphGDNDCdS",
        "outputId": "a19c363e-8e6a-454a-94f9-c06384f7a03e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['En', 'los', 'E.U.', 'esa', 'hamburguesa', 'cuesta', '$15.50']"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "(2) Lematizacion (encontrar la raiz linguistica de una palabra)\n",
        "\n",
        "Stemming = lematizacion simple"
      ],
      "metadata": {
        "id": "LqZ7vvrAEIAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "SnowballStemmer.languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-WJG9u5EPlV",
        "outputId": "205d0d90-b66a-4dfa-f4e3-9603ae9f4323"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('arabic',\n",
              " 'danish',\n",
              " 'dutch',\n",
              " 'english',\n",
              " 'finnish',\n",
              " 'french',\n",
              " 'german',\n",
              " 'hungarian',\n",
              " 'italian',\n",
              " 'norwegian',\n",
              " 'porter',\n",
              " 'portuguese',\n",
              " 'romanian',\n",
              " 'russian',\n",
              " 'spanish',\n",
              " 'swedish')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stem = SnowballStemmer('spanish')"
      ],
      "metadata": {
        "id": "8NBaDQrFFHrp"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stem.stem(\"trabajaremos\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZYBKglCEFQC3",
        "outputId": "8b59b6f3-5736-47c4-f222-717c86c6e9ed"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trabaj'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Lematizacion\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemm= WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "oFLQxiP1Fqmf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"wordnet\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B0PQVEVvF8Cl",
        "outputId": "268a5059-130b-479b-dd56-90c629ccaf3c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemm.lemmatize('trabajó')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7jz9kHSgGK12",
        "outputId": "8d2c1c13-30f5-4e48-d720-ea1f6cb20972"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'trabajó'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uso de Spacy en NLP"
      ],
      "metadata": {
        "id": "pjoF-eNiGn1d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**spacy** es una librería de procesamiento del lenguaje natural, robusta, rápida, fácil de instalar y utilizar e integrable con otras librerías de NLP y de deep learning.\n",
        "\n",
        "Tiene modelos entrenados en varios idiomas y permite realizar las típicas tareas de segmentación por oraciones, tokenizanción, análisis morfológico, extracción de entidades y análisis de opinión.\n",
        "\n",
        "Una vez instalados los modelos, podemos importarlos fácilmente:"
      ],
      "metadata": {
        "id": "RrU39xqKTR-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKYCTJ_8GyIs",
        "outputId": "44b011a8-3178-4011-c7be-b5f2fd95b921"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Descargamos el modelo de Spacy en español\n",
        "!python -m spacy download es"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVIJYU9eG7s0",
        "outputId": "2cd03755-bd48-4d15-a638-d66b260cf00e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-30 11:48:58.469952: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-08-30 11:49:01.522016: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'es' are deprecated. Please use the\n",
            "full pipeline package name 'es_core_news_sm' instead.\u001b[0m\n",
            "Collecting es-core-news-sm==3.6.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.7.0,>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from es-core-news-sm==3.6.0) (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.3)\n",
            "Installing collected packages: es-core-news-sm\n",
            "Successfully installed es-core-news-sm-3.6.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('es_core_news_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "OFe2jbIeHSos"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('es_core_news_sm')"
      ],
      "metadata": {
        "id": "gaCumO75HZYz"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Soy un texto.  Normalmente soy más largo y más grande.  Que no te engañe mi tamaño mayor a 10 palabras\""
      ],
      "metadata": {
        "id": "YPUUszhzIneC"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "1nvb-jGgI7BM"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Creamos una lista con los tokens del texto\n",
        "tokens = [t.orth_ for t in doc]\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-UerA9zJA-s",
        "outputId": "500c56de-e31c-4a8f-d90d-90cde4ce60f0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Soy',\n",
              " 'un',\n",
              " 'texto',\n",
              " '.',\n",
              " ' ',\n",
              " 'Normalmente',\n",
              " 'soy',\n",
              " 'más',\n",
              " 'largo',\n",
              " 'y',\n",
              " 'más',\n",
              " 'grande',\n",
              " '.',\n",
              " ' ',\n",
              " 'Que',\n",
              " 'no',\n",
              " 'te',\n",
              " 'engañe',\n",
              " 'mi',\n",
              " 'tamaño',\n",
              " 'mayor',\n",
              " 'a',\n",
              " '10',\n",
              " 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Limpiando el texto en Spacy**"
      ],
      "metadata": {
        "id": "gZPwEl6iJyar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Omitir:\n",
        "\n",
        "- Palabras comunes (y, o, ni, que)\n",
        "- Preposiciones (a, en, para, por, entre, otras)\n",
        "- Verbos (ser)\n",
        "- Las puntuaciones\n",
        "\n",
        "Todas aquellas palabras que no aportan un significado importante en el texto las denominaremos stopwords"
      ],
      "metadata": {
        "id": "IxdPFIOXKgrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_validos = [t.orth_ for t in doc if (not t.is_punct | t.is_stop) and t.orth_ != ' ']\n",
        "tokens_validos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmr2Sd5sKutb",
        "outputId": "be95d3f8-d992-4bc7-8715-bb2b9274f374"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['texto', 'Normalmente', 'engañe', 'tamaño', '10', 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Normalizar texto en Spacy**"
      ],
      "metadata": {
        "id": "jXidc8udKmrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = [t.lower() for t in tokens_validos if len(t)>3 and t.isalpha()]\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLra4LqmJ2pr",
        "outputId": "40070884-ed92-47bf-b90d-3e8b6a74d68a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['texto', 'normalmente', 'engañe', 'tamaño', 'palabras']"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0B6zvU80O0ef"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Similitud semántica entre palabras, frases y documentos**\n",
        "\n",
        "spaCy permite calcular la similitud semántica entre cualquier par de objetos de tipo Doc, Span o Token.\n",
        "\n",
        "Ojo, La similitud semántica es un concepto algo subjetivo, pero en este caso se puede entender como la probabilidad de que dos palabras aparezcan en los mismos contextos."
      ],
      "metadata": {
        "id": "xr7CNuUcXlm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "a90n_8y3W_bO"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_en = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "fCwXNGTnXAzP"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analizamos algunas colocaciones en inglés\n",
        "token1, _, token2 = nlp_en(\"cats and dogs\")\n",
        "token3, _, token4 = nlp_en(\"research and development\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUjA16sFYPWk",
        "outputId": "049669ca-4dc9-4548-c9a4-961cba66632c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cats vs dogs 0.34804314374923706\n",
            "research vs development 0.1919044852256775\n",
            "cats vs development 0.14609646797180176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-de2381c05edf>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "<ipython-input-36-de2381c05edf>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "<ipython-input-36-de2381c05edf>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ¿qué tal funciona en español?\n",
        "token1, _, token2 = nlp(\"perros y gatos\")\n",
        "token3, _, token4 = nlp(\"investigación y desarrollo\")\n",
        "\n",
        "# medimos la similitud semántica entre algunos pares\n",
        "print(token1, \"vs\", token2, token1.similarity(token2))\n",
        "print(token3, \"vs\", token4, token3.similarity(token4))\n",
        "print(token1, \"vs\", token4, token1.similarity(token4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Yac1-xYYPkT",
        "outputId": "eedb41ab-8a65-41a8-bd71-ba02f16147cc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perros vs gatos 0.6648734211921692\n",
            "investigación vs desarrollo 0.182033509016037\n",
            "perros vs desarrollo -0.03289707377552986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-a9925c3ae975>:6: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token2, token1.similarity(token2))\n",
            "<ipython-input-37-a9925c3ae975>:7: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token3, \"vs\", token4, token3.similarity(token4))\n",
            "<ipython-input-37-a9925c3ae975>:8: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1, \"vs\", token4, token1.similarity(token4))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f_O5Df5_XMNi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}